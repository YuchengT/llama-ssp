TODO
- Benchmarks in token/s for 7B, 13B quantized, 7B non-quantized
    - display model results in markdown
- test avec un 7/13 sur la machine 4GPUs
    - puis un 7/30 car là le speedup sera p-e présent
    - 2 modes : All-4 GPUs (easy but slow draft) ou 1+3GPUs (draft should then be faster than target, better speedup)
- sujet du batching pour le timing
    - handle batch > 1
    - monter la batch size pour obtenir une vraie diff
    - refactor & clean output: tokens/s for various settings (+ commit)
- experiments on speedup
- handle max context size
- test with a temperature / top_p sampling 
- sampling method as a param
- handle stop token
- clean implementation
- timing issues : pourquoi le manual sampling est plus long que le sampling lib HF?
- debug mem reqs pour voir si monoGPU possible
