/home/ubuntu/anaconda3/envs/llama_ssp/lib/python3.9/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. 
The class this function is called from is 'LlamaTokenizer'.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:03<00:22,  3.69s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:07<00:18,  3.60s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:10<00:14,  3.65s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:14<00:11,  3.70s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:18<00:07,  3.69s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:22<00:03,  3.87s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:25<00:00,  3.61s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:25<00:00,  3.67s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.42s/it]
{'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 2, 'model.layers.12': 2, 'model.layers.13': 2, 'model.layers.14': 2, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 3, 'model.layers.18': 3, 'model.layers.19': 3, 'model.layers.20': 3, 'model.layers.21': 3, 'model.layers.22': 3, 'model.layers.23': 4, 'model.layers.24': 4, 'model.layers.25': 4, 'model.layers.26': 4, 'model.layers.27': 4, 'model.layers.28': 4, 'model.layers.29': 5, 'model.layers.30': 5, 'model.layers.31': 5, 'model.layers.32': 5, 'model.layers.33': 5, 'model.layers.34': 5, 'model.layers.35': 6, 'model.layers.36': 6, 'model.layers.37': 6, 'model.layers.38': 6, 'model.layers.39': 6, 'model.layers.40': 6, 'model.layers.41': 7, 'model.layers.42': 7, 'model.layers.43': 7, 'model.layers.44': 7, 'model.layers.45': 7, 'model.layers.46': 7, 'model.layers.47': 7, 'model.norm': 7, 'lm_head': 7}
{'': 0}
Warming up
Comparing 34B_code_8bit model regular sampling and 34B_code_8bit SSp with 7B_code_4bit draft model
====

=> Regular sampling with target model
from typing import List


def has_close_elements(numbers: List[float], threshold: float) -> bool:
    """ Check if in given list of numbers, are any two numbers closer to each other than
    given threshold.
    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)
    False
    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)
    True
    """
    for i in range(len(numbers)):
        for j in range(i + 1, len(numbers)):
            if abs(numbers[i] - numbers[j]) < threshold:
                return True
    return False


if __name__ == "__main__":
    from doctest import testmod

    testmod()
/home/ubuntu/anaconda3/envs/llama_ssp/lib/python3.9/site-packages/bitsandbytes/nn/modules.py:224: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.
  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')

Time: 36.61s
=> Speculative sampling with target model helped by draft model
from typing import List


def has_close_elements(numbers: List[float], threshold: float) -> bool:
    """ Check if in given list of numbers, are any two numbers closer to each other than
    given threshold.
    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)
    False
    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)
    True
    """
    for i in range(len(numbers)):
        for j in range(i + 1, len(numbers)):
            if abs(numbers[i] - numbers[j]) < threshold:
                return True
    return False


if __name__ == "__main__":
    import doctest

    doctest.testmod()

Time: 32.11s
Acceptance Rate: 0.9558823529411765
(tensor([0.9922]), tensor([0.9877]), tensor([0.9899]), tensor([0.9882]))
=> Regular sampling with target model
from typing import List


def separate_paren_groups(paren_string: str) -> List[str]:
    """ Input to this function is a string containing multiple groups of nested parentheses. Your goal is to
    separate those group into separate strings and return the list of those.
    Separate groups are balanced (each open brace is properly closed) and not nested within each other
    Ignore any spaces in the input string.
    >>> separate_paren_groups('( ) (( )) (( )( ))')
    ['()', '(())', '(()())']
    """
    assert paren_string.count("(") == paren_string.count(")"), "Unbalanced parens"
    result = []
    current_group = []
    current_depth = 0
    for char in paren_string:
        if char == "(":
            current_depth += 1
        if char == ")":
            current_depth -= 1
        if char in "()
Time: 39.08s
=> Speculative sampling with target model helped by draft model
from typing import List


def separate_paren_groups(paren_string: str) -> List[str]:
    """ Input to this function is a string containing multiple groups of nested parentheses. Your goal is to
    separate those group into separate strings and return the list of those.
    Separate groups are balanced (each open brace is properly closed) and not nested within each other
    Ignore any spaces in the input string.
    >>> separate_paren_groups('( ) (( )) (( )( ))')
    ['()', '(())', '(()())']
    """
    assert paren_string.count("(") == paren_string.count(")"), "Unbalanced parens"
    result = []
    current_group = []
    current_depth = 0
    for char in paren_string:
        if char == "(":
            current_depth += 1
        if char == ")":
            current_depth -= 1
        if char in "()":
            if
Time: 49.90s
Acceptance Rate: 0.7115384615384616
(tensor([0.9863]), tensor([0.9876]), tensor([0.9870]), tensor([0.9875]))
=> Regular sampling with target model


def truncate_number(number: float) -> float:
    """ Given a positive floating point number, it can be decomposed into
    and integer part (largest integer smaller than given number) and decimals
    (leftover part always smaller than 1).

    Return the decimal part of the number.
    >>> truncate_number(3.5)
    0.5
    """
    return number - int(number)


def round_half_up(number: float) -> int:
    """ Given a positive floating point number, it can be decomposed into
    and integer part (largest integer smaller than given number) and decimals
    (leftover part always smaller than 1).

    When the decimal part is exactly 0.5 we can round up or down (i.e.
Time: 38.66s
=> Speculative sampling with target model helped by draft model


def truncate_number(number: float) -> float:
    """ Given a positive floating point number, it can be decomposed into
    and integer part (largest integer smaller than given number) and decimals
    (leftover part always smaller than 1).

    Return the decimal part of the number.
    >>> truncate_number(3.5)
    0.5
    """
    return number - int(number)


print(truncate_number(3.5))
 import
Time: 8.57s
Acceptance Rate: 0.875
(tensor([0.9334]), tensor([0.8816]), tensor([0.9068]), tensor([0.8866]))
=> Regular sampling with target model
from typing import List


def below_zero(operations: List[int]) -> bool:
    """ You're given a list of deposit and withdrawal operations on a bank account that starts with
    zero balance. Your task is to detect if at any point the balance of account fallls below zero, and
    at that point function should return True. Otherwise it should return False.
    >>> below_zero([1, 2, 3])
    False
    >>> below_zero([1, 2, -4, 5])
    True
    """

    balance = 0

    for operation in operations:
        balance += operation
        if balance < 0:
            return True

    return False


if __name__ == "__main__":
    print(below_zero([1, 2, 3]))
    print(below_zero([1, 2, -4, 5]))

Time: 35.06s
=> Speculative sampling with target model helped by draft model
from typing import List


def below_zero(operations: List[int]) -> bool:
    """ You're given a list of deposit and withdrawal operations on a bank account that starts with
    zero balance. Your task is to detect if at any point the balance of account fallls below zero, and
    at that point function should return True. Otherwise it should return False.
    >>> below_zero([1, 2, 3])
    False
    >>> below_zero([1, 2, -4, 5])
    True
    """
    balance = 0
    for x in operations:
        balance += x
        if balance < 0:
            return True
    return False


if __name__ == "__main__":
    import doctest

    doctest.testmod()

Time: 22.07s
Acceptance Rate: 0.9807692307692307
(tensor([0.9689]), tensor([0.9751]), tensor([0.9720]), tensor([0.9745]))
=> Regular sampling with target model
from typing import List


def mean_absolute_deviation(numbers: List[float]) -> float:
    """ For a given list of input numbers, calculate Mean Absolute Deviation
    around the mean of this dataset.
    Mean Absolute Deviation is the average absolute difference between each
    element and a centerpoint (mean in this case):
    MAD = average | x - x_mean |
    >>> mean_absolute_deviation([1.0, 2.0, 3.0, 4.0])
    1.0
    """
    mean = sum(numbers) / len(numbers)
    absolute_differences = [abs(mean - x) for x in numbers]
    return sum(absolute_differences) / len(numbers)


def standard_deviation(numbers: List[float]) -> float:
    """ For a given list of input numbers, calculate the standard deviation
    around the mean of this dataset.
    Standard Deviation is
Time: 38.89s
=> Speculative sampling with target model helped by draft model
from typing import List


def mean_absolute_deviation(numbers: List[float]) -> float:
    """ For a given list of input numbers, calculate Mean Absolute Deviation
    around the mean of this dataset.
    Mean Absolute Deviation is the average absolute difference between each
    element and a centerpoint (mean in this case):
    MAD = average | x - x_mean |
    >>> mean_absolute_deviation([1.0, 2.0, 3.0, 4.0])
    1.0
    """
    mean = sum(numbers) / len(numbers)
    absolute_differences = [abs(mean - x) for x in numbers]
    return sum(absolute_differences) / len(numbers)


def standard_deviation(numbers: List[float]) -> float:
    """ For a given list of input numbers, calculate standard deviation
    around the mean of this dataset.
    Standard Deviation is square
Time: 38.53s
Acceptance Rate: 0.8928571428571429
(tensor([0.9928]), tensor([0.9942]), tensor([0.9935]), tensor([0.9940]))
=> Regular sampling with target model
from typing import List


def intersperse(numbers: List[int], delimeter: int) -> List[int]:
    """ Insert a number 'delimeter' between every two consecutive elements of input list `numbers'
    >>> intersperse([], 4)
    []
    >>> intersperse([1, 2, 3], 4)
    [1, 4, 2, 4, 3]
    """
    result = []
    for index in range(len(numbers) - 1):
        result.append(numbers[index])
        result.append(delimeter)
    result.append(numbers[-1])
    return result


def intersperse_with_ifs(numbers: List[int], delimeter: int) -> List[int]:
    """ Insert a number 'delimeter' between every two consecutive elements
Time: 38.75s
=> Speculative sampling with target model helped by draft model
from typing import List


def intersperse(numbers: List[int], delimeter: int) -> List[int]:
    """ Insert a number 'delimeter' between every two consecutive elements of input list `numbers'
    >>> intersperse([], 4)
    []
    >>> intersperse([1, 2, 3], 4)
    [1, 4, 2, 4, 3]
    """
    result = []
    for index in range(len(numbers) - 1):
        result.append(numbers[index])
        result.append(delimeter)
    result.append(numbers[-1])
    return result


if __name__ == "__main__":
    from doctest import testmod

    testmod()

Time: 29.72s
Acceptance Rate: 0.9027777777777778
(tensor([0.9374]), tensor([0.9472]), tensor([0.9422]), tensor([0.9462]))
=> Regular sampling with target model
from typing import List


def parse_nested_parens(paren_string: str) -> List[int]:
    """ Input to this function is a string represented multiple groups for nested parentheses separated by spaces.
    For each of the group, output the deepest level of nesting of parentheses.
    E.g. (()()) has maximum two levels of nesting while ((())) has three.

    >>> parse_nested_parens('(()()) ((())) () ((())()())')
    [2, 3, 1, 3]
    """
    max_depths = []
    for paren_group in paren_string.split():
        depth = 0
        max_depth = 0
        for paren in paren_group:
            if paren == '(':
                depth += 1
                max_depth = max(max_depth, depth)
            elif paren == ')':
                depth -= 1
            else:
               
Time: 38.89s
=> Speculative sampling with target model helped by draft model
from typing import List


def parse_nested_parens(paren_string: str) -> List[int]:
    """ Input to this function is a string represented multiple groups for nested parentheses separated by spaces.
    For each of the group, output the deepest level of nesting of parentheses.
    E.g. (()()) has maximum two levels of nesting while ((())) has three.

    >>> parse_nested_parens('(()()) ((())) () ((())()())')
    [2, 3, 1, 3]
    """
    max_depths = []
    for paren_group in paren_string.split():
        depth = 0
        max_depth = 0
        for paren in paren_group:
            if paren == '(':
                depth += 1
                max_depth = max(max_depth, depth)
            elif paren == ')':
                depth -= 1
        max_depths
Time: 37.41s
Acceptance Rate: 0.95
(tensor([0.9940]), tensor([0.9959]), tensor([0.9949]), tensor([0.9957]))
=> Regular sampling with target model
from typing import List


def filter_by_substring(strings: List[str], substring: str) -> List[str]:
    """ Filter an input list of strings only for ones that contain given substring
    >>> filter_by_substring([], 'a')
    []
    >>> filter_by_substring(['abc', 'bacd', 'cde', 'array'], 'a')
    ['abc', 'bacd', 'array']
    """
    return [string for string in strings if substring in string]


if __name__ == "__main__":
    from doctest import testmod

    testmod()

Time: 17.23s
=> Speculative sampling with target model helped by draft model
from typing import List


def filter_by_substring(strings: List[str], substring: str) -> List[str]:
    """ Filter an input list of strings only for ones that contain given substring
    >>> filter_by_substring([], 'a')
    []
    >>> filter_by_substring(['abc', 'bacd', 'cde', 'array'], 'a')
    ['abc', 'bacd', 'array']
    """
    return [string for string in strings if substring in string]


if __name__ == "__main__":
    from doctest import testmod

    testmod()

Time: 13.81s
Acceptance Rate: 0.9722222222222222
(tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]))
=> Regular sampling with target model
from typing import List, Tuple


def sum_product(numbers: List[int]) -> Tuple[int, int]:
    """ For a given list of integers, return a tuple consisting of a sum and a product of all the integers in a list.
    Empty sum should be equal to 0 and empty product should be equal to 1.
    >>> sum_product([])
    (0, 1)
    >>> sum_product([1, 2, 3, 4])
    (10, 24)
    """
    return sum(numbers), reduce(lambda x, y: x * y, numbers, 1)


if __name__ == "__main__":
    import doctest

    doctest.testmod()

Time: 21.44s
=> Speculative sampling with target model helped by draft model
from typing import List, Tuple


def sum_product(numbers: List[int]) -> Tuple[int, int]:
    """ For a given list of integers, return a tuple consisting of a sum and a product of all the integers in a list.
    Empty sum should be equal to 0 and empty product should be equal to 1.
    >>> sum_product([])
    (0, 1)
    >>> sum_product([1, 2, 3, 4])
    (10, 24)
    """
    return sum(numbers), reduce(lambda x, y: x * y, numbers, 1)


def sum_product2(numbers: List[int]) -> Tuple[int, int]:
    return sum(numbers), reduce(lambda x, y: x * y, numbers or [1], 1)


def sum_product3(numbers: List[int]) -> Tuple[int, int]:
    return sum(numbers
Time: 39.01s
Acceptance Rate: 0.9404761904761905
(tensor([0.9438]), tensor([0.9510]), tensor([0.9474]), tensor([0.9503]))
=> Regular sampling with target model
from typing import List, Tuple


def rolling_max(numbers: List[int]) -> List[int]:
    """ From a given list of integers, generate a list of rolling maximum element found until given moment
    in the sequence.
    >>> rolling_max([1, 2, 3, 2, 3, 4, 2])
    [1, 2, 3, 3, 3, 4, 4]
    """
    result = []
    max_number = -1000000000000000000000000000000000000000000000000000000000000000000000000000000000000
Time: 38.62s
=> Speculative sampling with target model helped by draft model
from typing import List, Tuple


def rolling_max(numbers: List[int]) -> List[int]:
    """ From a given list of integers, generate a list of rolling maximum element found until given moment
    in the sequence.
    >>> rolling_max([1, 2, 3, 2, 3, 4, 2])
    [1, 2, 3, 3, 3, 4, 4]
    """
    rolling_max_list = []
    max_number = -1000000000000000000000000000000000000000000000000000000000000000000000000000000000
Time: 37.01s
Acceptance Rate: 0.8522727272727273
(tensor([0.9864]), tensor([0.9883]), tensor([0.9874]), tensor([0.9881]))
=> Regular sampling with target model


def is_palindrome(string: str) -> bool:
    """ Test if given string is a palindrome """
    return string == string[::-1]


def make_palindrome(string: str) -> str:
    """ Find the shortest palindrome that begins with a supplied string.
    Algorithm idea is simple:
    - Find the longest postfix of supplied string that is a palindrome.
    - Append to the end of the string reverse of a string prefix that comes before the palindromic suffix.
    >>> make_palindrome('')
    ''
    >>> make_palindrome('cat')
    'catac'
    >>> make_palindrome('cata')
    'catac'
    """
    # Find the longest postfix that is a palindrome.
    for i in range(len(string)):
        suffix = string[i:]
        if is_palindrome(suffix):
            # suffix is a palindrome! Return it plus a prefix before in reverse order.
            return suffix + string[:i][::-1]
    # A special case for an empty string.
    return ''

Time: 39.68s
=> Speculative sampling with target model helped by draft model


def is_palindrome(string: str) -> bool:
    """ Test if given string is a palindrome """
    return string == string[::-1]


def make_palindrome(string: str) -> str:
    """ Find the shortest palindrome that begins with a supplied string.
    Algorithm idea is simple:
    - Find the longest postfix of supplied string that is a palindrome.
    - Append to the end of the string reverse of a string prefix that comes before the palindromic suffix.
    >>> make_palindrome('')
    ''
    >>> make_palindrome('cat')
    'catac'
    >>> make_palindrome('cata')
    'catac'
    """
    suffix_length = 0
    for i in range(1, len(string) + 1):
        if is_palindrome(string[-i:]):
            suffix_length = i

    return string + string[:suffix_length][::-1]

Time: 35.79s
Acceptance Rate: 0.7058823529411765
(tensor([0.9680]), tensor([0.9433]), tensor([0.9555]), tensor([0.9458]))
=> Regular sampling with target model
from typing import List


def string_xor(a: str, b: str) -> str:
    """ Input are two strings a and b consisting only of 1s and 0s.
    Perform binary XOR on these inputs and return result also as a string.
    >>> string_xor('010', '110')
    '100'
    """
    return "".join(str(int(c1 != c2)) for c1, c2 in zip(a, b))


def single_number(nums: List[int]) -> int:
    """ Given a non-empty array of integers, every element appears twice except for one. Find that single one.
    >>> single_number([2,2,1])
    1
    """
    result = 
Time: 38.57s
=> Speculative sampling with target model helped by draft model
from typing import List


def string_xor(a: str, b: str) -> str:
    """ Input are two strings a and b consisting only of 1s and 0s.
    Perform binary XOR on these inputs and return result also as a string.
    >>> string_xor('010', '110')
    '100'
    """
    return "".join(str(int(c1 != c2)) for c1, c2 in zip(a, b))


def single_number(nums: List[int]) -> int:
    """ Given a non-empty array of integers, every element appears twice except for one. Find that single one.
    >>> single_number([2,2,1])
    1
    """
    result = 0
   
Time: 38.23s
Acceptance Rate: 0.78125
(tensor([0.9918]), tensor([0.9941]), tensor([0.9930]), tensor([0.9939]))
=> Regular sampling with target model
from typing import List, Optional


def longest(strings: List[str]) -> Optional[str]:
    """ Out of list of strings, return the longest one. Return the first one in case of multiple
    strings of the same length. Return None in case the input list is empty.
    >>> longest([])

    >>> longest(['a', 'b', 'c'])
    'a'
    >>> longest(['a', 'bb', 'ccc'])
    'ccc'
    """
    if not strings:
        return None
    return max(strings, key=len)


METADATA = {}


def check(candidate):
    assert candidate([]) == None
    assert candidate(["x"]) == "x"
    assert candidate(["one", "two", "three", "four", "five"]) == "three"
    assert candidate(["one", "two", "three
Time: 38.81s
=> Speculative sampling with target model helped by draft model
from typing import List, Optional


def longest(strings: List[str]) -> Optional[str]:
    """ Out of list of strings, return the longest one. Return the first one in case of multiple
    strings of the same length. Return None in case the input list is empty.
    >>> longest([])

    >>> longest(['a', 'b', 'c'])
    'a'
    >>> longest(['a', 'bb', 'ccc'])
    'ccc'
    """
    if not strings:
        return None
    return max(strings, key=len)


print(longest([]))
print(longest(['a', 'b', 'c']))
print(longest(['a', 'bb', 'ccc']))
 import
Time: 22.51s
Acceptance Rate: 0.8571428571428571
(tensor([0.9447]), tensor([0.8928]), tensor([0.9180]), tensor([0.8978]))
=> Regular sampling with target model


def greatest_common_divisor(a: int, b: int) -> int:
    """ Return a greatest common divisor of two integers a and b
    >>> greatest_common_divisor(3, 5)
    1
    >>> greatest_common_divisor(25, 15)
    5
    """
    while b:
        a, b = b, a % b
    return a


if __name__ == "__main__":
    import doctest
    doctest.testmod()

Time: 19.75s
=> Speculative sampling with target model helped by draft model


def greatest_common_divisor(a: int, b: int) -> int:
    """ Return a greatest common divisor of two integers a and b
    >>> greatest_common_divisor(3, 5)
    1
    >>> greatest_common_divisor(25, 15)
    5
    """
    while b:
        a, b = b, a % b
    return a


if __name__ == "__main__":
    import doctest
    doctest.testmod()

Time: 15.01s
Acceptance Rate: 0.8636363636363636
(tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]))
=> Regular sampling with target model
from typing import List


def all_prefixes(string: str) -> List[str]:
    """ Return list of all prefixes from shortest to longest of the input string
    >>> all_prefixes('abc')
    ['a', 'ab', 'abc']
    """
    return [string[:i] for i in range(len(string) + 1)]


def all_suffixes(string: str) -> List[str]:
    """ Return list of all suffixes from shortest to longest of the input string
    >>> all_suffixes('abc')
    ['abc', 'bc', 'c', '']
    """
    return [string[i:] for i
Time: 38.36s
=> Speculative sampling with target model helped by draft model
from typing import List


def all_prefixes(string: str) -> List[str]:
    """ Return list of all prefixes from shortest to longest of the input string
    >>> all_prefixes('abc')
    ['a', 'ab', 'abc']
    """
    return [string[: i + 1] for i in range(len(string))]


def all_suffixes(string: str) -> List[str]:
    """ Return list of all suffixes from shortest to longest of the input string
    >>> all_suffixes('abc')
    ['abc', 'bc', 'c', '']
    """
    return [string[-i:] for i in range(
Time: 33.40s
Acceptance Rate: 0.8152173913043478
(tensor([0.9822]), tensor([0.9901]), tensor([0.9861]), tensor([0.9893]))
=> Regular sampling with target model


def string_sequence(n: int) -> str:
    """ Return a string containing space-delimited numbers starting from 0 upto n inclusive.
    >>> string_sequence(0)
    '0'
    >>> string_sequence(5)
    '0 1 2 3 4 5'
    """
    return ' '.join(str(i) for i in range(n + 1))


def count_words(s: str) -> int:
    """ Return the number of words in s.
    >>> count_words('This is a test')
    4
    """
    return len(s.split())


def count_up(n: int) -> None:
    """ Print integers from 0
Time: 38.57s
=> Speculative sampling with target model helped by draft model


def string_sequence(n: int) -> str:
    """ Return a string containing space-delimited numbers starting from 0 upto n inclusive.
    >>> string_sequence(0)
    '0'
    >>> string_sequence(5)
    '0 1 2 3 4 5'
    """
    return ' '.join(str(i) for i in range(n + 1))


def count_words(s: str) -> int:
    """ Return the number of words in s. Words are separated by one or more spaces.
    >>> count_words('Hello, World!')
    2
    """
    return len(s.split())


def count_up(n: int) -> int
Time: 35.81s
Acceptance Rate: 0.8043478260869565
(tensor([0.9570]), tensor([0.9602]), tensor([0.9586]), tensor([0.9599]))
=> Regular sampling with target model


def count_distinct_characters(string: str) -> int:
    """ Given a string, find out how many distinct characters (regardless of case) does it consist of
    >>> count_distinct_characters('xyzXYZ')
    3
    >>> count_distinct_characters('Jerry')
    4
    """
    return len(set(string.lower()))


def count_distinct_characters_faster(string: str) -> int:
    """ Given a string, find out how many distinct characters (regardless of case) does it consist of
    >>> count_distinct_characters_faster('xyzXYZ')
    3
    >>> count_distinct_characters_faster('Jerry')

Time: 38.68s
=> Speculative sampling with target model helped by draft model


def count_distinct_characters(string: str) -> int:
    """ Given a string, find out how many distinct characters (regardless of case) does it consist of
    >>> count_distinct_characters('xyzXYZ')
    3
    >>> count_distinct_characters('Jerry')
    4
    """
    return len(set(string.lower()))


def convert_to_hours(seconds: int) -> int:
    """ Given a time in seconds, return the number of hours it represents
    >>> convert_to_hours(3900)
    1
    >>> convert_to_hours(3600)
    1
    """
    return seconds // 3600


def get
Time: 35.75s
Acceptance Rate: 0.8152173913043478
(tensor([0.8727]), tensor([0.9128]), tensor([0.8923]), tensor([0.9086]))
=> Regular sampling with target model
from typing import List


def parse_music(music_string: str) -> List[int]:
    """ Input to this function is a string representing musical notes in a special ASCII format.
    Your task is to parse this string and return list of integers corresponding to how many beats does each
    not last.

    Here is a legend:
    'o' - whole note, lasts four beats
    'o|' - half note, lasts two beats
    '.|' - quater note, lasts one beat

    >>> parse_music('o o| .| o| o| .| .| .| .| o o')
    [4, 2, 1, 2, 2, 1, 1, 1, 1, 4, 4]
    """
    note_map = {
        'o': 4,
        'o|': 2,
        '.|': 1
    }
    return [note_map[note] for note in music_string.split()]


def parse_music_hard(music_string: str) -> List[int]:
    """ Input to this function is a string representing musical notes in a special ASCII format.
    Your
Time: 39.95s
=> Speculative sampling with target model helped by draft model
from typing import List


def parse_music(music_string: str) -> List[int]:
    """ Input to this function is a string representing musical notes in a special ASCII format.
    Your task is to parse this string and return list of integers corresponding to how many beats does each
    not last.

    Here is a legend:
    'o' - whole note, lasts four beats
    'o|' - half note, lasts two beats
    '.|' - quater note, lasts one beat

    >>> parse_music('o o| .| o| o| .| .| .| .| o o')
    [4, 2, 1, 2, 2, 1, 1, 1, 1, 4, 4]
    """
    note_map = {
        'o': 4,
        'o|': 2,
        '.|': 1
    }
    return [note_map[note] for note in music_string.split()]


def count_espressos(cups_of_coffee: int, caffeine_per_cup: List[int]) -> int:
    """ You drink coffee,
Time: 60.02s
Acceptance Rate: 0.6826923076923077
(tensor([0.9405]), tensor([0.9610]), tensor([0.9506]), tensor([0.9589]))
=> Regular sampling with target model


def how_many_times(string: str, substring: str) -> int:
    """ Find how many times a given substring can be found in the original string. Count overlaping cases.
    >>> how_many_times('', 'a')
    0
    >>> how_many_times('aaa', 'a')
    3
    >>> how_many_times('aaaa', 'aa')
    3
    """
    counter = 0
    for i in range(len(string)):
        if string[i:i+len(substring)] == substring:
            counter += 1
    return counter


if __name__ == '__main__':
    import doctest
    doctest.testmod()

Time: 29.08s
=> Speculative sampling with target model helped by draft model


def how_many_times(string: str, substring: str) -> int:
    """ Find how many times a given substring can be found in the original string. Count overlaping cases.
    >>> how_many_times('', 'a')
    0
    >>> how_many_times('aaa', 'a')
    3
    >>> how_many_times('aaaa', 'aa')
    3
    """
    counter = 0
    for i in range(len(string)):
        if string[i:i+len(substring)] == substring:
            counter += 1
    return counter


if __name__ == '__main__':
    import doctest
    doctest.testmod()

Time: 23.57s
Acceptance Rate: 0.95
(tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]))
=> Regular sampling with target model
from typing import List


def sort_numbers(numbers: str) -> str:
    """ Input is a space-delimited string of numberals from 'zero' to 'nine'.
    Valid choices are 'zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight' and 'nine'.
    Return the string with numbers sorted from smallest to largest
    >>> sort_numbers('three one five')
    'one three five'
    """
    return " ".join(sorted(numbers.split(), key=lambda x: ["zero", "one", "two", "three", "four", "five", "six", "seven", "eight", "nine"].index(x))


def sort_numbers_with_mixed_types(numbers: List) -> List:
    """ Input is a list of strings and ints from 0 to 9.

Time: 38.81s
=> Speculative sampling with target model helped by draft model
from typing import List


def sort_numbers(numbers: str) -> str:
    """ Input is a space-delimited string of numberals from 'zero' to 'nine'.
    Valid choices are 'zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight' and 'nine'.
    Return the string with numbers sorted from smallest to largest
    >>> sort_numbers('three one five')
    'one three five'
    """
    return " ".join(sorted(numbers.split(), key=lambda x: ["zero", "one", "two", "three", "four", "five", "six", "seven", "eight", "nine"].index(x))


def sort_numbers_with_mixed_types(numbers: List) -> List:
    """ Input is a list of strings and ints from 0 to 9.
    Valid
Time: 46.91s
Acceptance Rate: 0.6923076923076923
(tensor([0.9937]), tensor([0.9961]), tensor([0.9949]), tensor([0.9958]))
=> Regular sampling with target model
from typing import List, Tuple


def find_closest_elements(numbers: List[float]) -> Tuple[float, float]:
    """ From a supplied list of numbers (of length at least two) select and return two that are the closest to each
    other and return them in order (smaller number, larger number).
    >>> find_closest_elements([1.0, 2.0, 3.0, 4.0, 5.0, 2.2])
    (2.0, 2.2)
    >>> find_closest_elements([1.0, 2.0, 3.0, 4.0, 5.0, 2.0])
    (2.0, 2.0)
    """
    closest_pair = None
    closest_distance = None
    for i in range(len(numbers) - 1):
        for j in range(i + 1, len(numbers)):
            distance = abs(numbers[i] - numbers[j])
            if closest_distance is None or distance < closest_distance:
                closest_distance = distance
                closest_pair = (numbers[i], numbers[j])
Time: 39.71s
=> Speculative sampling with target model helped by draft model
from typing import List, Tuple


def find_closest_elements(numbers: List[float]) -> Tuple[float, float]:
    """ From a supplied list of numbers (of length at least two) select and return two that are the closest to each
    other and return them in order (smaller number, larger number).
    >>> find_closest_elements([1.0, 2.0, 3.0, 4.0, 5.0, 2.2])
    (2.0, 2.2)
    >>> find_closest_elements([1.0, 2.0, 3.0, 4.0, 5.0, 2.0])
    (2.0, 2.0)
    """
    closest_pair = None
    closest_distance = None
    for i in range(len(numbers) - 1):
        for j in range(i + 1, len(numbers)):
            distance = abs(numbers[i] - numbers[j])
            if closest_distance is None or distance < closest_distance:
                closest_distance = distance
                closest_pair = (min(numbers[i], numbers[j]), max
Time: 48.45s
Acceptance Rate: 0.875
(tensor([0.9884]), tensor([0.9937]), tensor([0.9911]), tensor([0.9932]))
=> Regular sampling with target model
from typing import List


def rescale_to_unit(numbers: List[float]) -> List[float]:
    """ Given list of numbers (of at least two elements), apply a linear transform to that list,
    such that the smallest number will become 0 and the largest will become 1
    >>> rescale_to_unit([1.0, 2.0, 3.0, 4.0, 5.0])
    [0.0, 0.25, 0.5, 0.75, 1.0]
    """
    if len(numbers) < 2:
        raise ValueError("Cannot rescale a list of less than 2 numbers")
    min_value = min(numbers)
    max_value = max(numbers)
    return [(x - min_value) / (max_value - min_value) for x in numbers]

Time: 31.14s
=> Speculative sampling with target model helped by draft model
from typing import List


def rescale_to_unit(numbers: List[float]) -> List[float]:
    """ Given list of numbers (of at least two elements), apply a linear transform to that list,
    such that the smallest number will become 0 and the largest will become 1
    >>> rescale_to_unit([1.0, 2.0, 3.0, 4.0, 5.0])
    [0.0, 0.25, 0.5, 0.75, 1.0]
    """
    if len(numbers) < 2:
        raise ValueError("List must have at least two elements")
    min_value = min(numbers)
    max_value = max(numbers)
    return [(x - min_value) / (max_value - min_value) for x in numbers]

Time: 27.42s
Acceptance Rate: 0.95
(tensor([0.9889]), tensor([0.9808]), tensor([0.9848]), tensor([0.9816]))
=> Regular sampling with target model
from typing import List, Any


def filter_integers(values: List[Any]) -> List[int]:
    """ Filter given list of any python values only for integers
    >>> filter_integers(['a', 3.14, 5])
    [5]
    >>> filter_integers([1, 2, 3, 'abc', {}, []])
    [1, 2, 3]
    """
    return list(filter(lambda x: isinstance(x, int), values))


if __name__ == '__main__':
    #These "asserts" using only for self-checking and not necessary for auto-testing
    assert filter_integers([]) == [], "Empty list"
    assert filter_integers([1, 2, 3]) == [1, 2, 3], "
Time: 38.91s
=> Speculative sampling with target model helped by draft model
from typing import List, Any


def filter_integers(values: List[Any]) -> List[int]:
    """ Filter given list of any python values only for integers
    >>> filter_integers(['a', 3.14, 5])
    [5]
    >>> filter_integers([1, 2, 3, 'abc', {}, []])
    [1, 2, 3]
    """
    return list(filter(lambda x: isinstance(x, int), values))


if __name__ == '__main__':
    #These "asserts" using only for self-checking and not necessary for auto-testing
    assert filter_integers([]) == [], "Nothing to filter"
    assert filter_integers([1, 2, 3, "a", "b", 4, 
Time: 38.84s
Acceptance Rate: 0.7934782608695652
(tensor([0.9721]), tensor([0.9811]), tensor([0.9766]), tensor([0.9802]))
=> Regular sampling with target model


def strlen(string: str) -> int:
    """ Return length of given string
    >>> strlen('')
    0
    >>> strlen('abc')
    3
    """
    length = 0
    for _ in string:
        length += 1
    return length


def is_palindrome(string: str) -> bool:
    """ Return True if given string is palindrome
    >>> is_palindrome('')
    True
    >>> is_palindrome('a')
    True
    >>> is_palindrome('ab')
    False
    >>> is
Time: 38.42s
=> Speculative sampling with target model helped by draft model


def strlen(string: str) -> int:
    """ Return length of given string
    >>> strlen('')
    0
    >>> strlen('abc')
    3
    """
    length = 0
    for _ in string:
        length += 1
    return length


if __name__ == '__main__':
    import doctest
    doctest.testmod()

Time: 12.95s
Acceptance Rate: 0.9318181818181818
(tensor([0.8857]), tensor([0.8135]), tensor([0.8481]), tensor([0.8202]))
=> Regular sampling with target model


def largest_divisor(n: int) -> int:
    """ For a given number n, find the largest number that divides n evenly, smaller than n
    >>> largest_divisor(15)
    5
    """
    if n < 10:
        return n
    for i in range(n//2, 1, -1):
        if n % i == 0:
            return i
    return 1


if __name__ == "__main__":
    import doctest
    doctest.testmod()
Time: 30.77s
=> Speculative sampling with target model helped by draft model


def largest_divisor(n: int) -> int:
    """ For a given number n, find the largest number that divides n evenly, smaller than n
    >>> largest_divisor(15)
    5
    """
    if n < 1:
        return 1
    for i in range(n, 0, -1):
        if n % i == 0:
            return i


if __name__ == "__main__":
    import doctest
    doctest.testmod()
Time: 21.73s
Acceptance Rate: 0.7794117647058824
(tensor([0.9869]), tensor([0.9745]), tensor([0.9807]), tensor([0.9757]))
=> Regular sampling with target model
from typing import List


def factorize(n: int) -> List[int]:
    """ Return list of prime factors of given integer in the order from smallest to largest.
    Each of the factors should be listed number of times corresponding to how many times it appeares in factorization.
    Input number should be equal to the product of all factors
    >>> factorize(8)
    [2, 2, 2]
    >>> factorize(25)
    [5, 5]
    >>> factorize(70)
    [2, 5, 7]
    """
    prime_factors = []
    i = 2
    while i * i <= n:
        if n % i:
            i += 1
        else:
            n //= i
            prime_factors.append(i)
    if n > 1:
        prime_factors.append(n)
    return prime_factors


if __name__ == "__main__":

Time: 39.38s
=> Speculative sampling with target model helped by draft model
from typing import List


def factorize(n: int) -> List[int]:
    """ Return list of prime factors of given integer in the order from smallest to largest.
    Each of the factors should be listed number of times corresponding to how many times it appeares in factorization.
    Input number should be equal to the product of all factors
    >>> factorize(8)
    [2, 2, 2]
    >>> factorize(25)
    [5, 5]
    >>> factorize(70)
    [2, 5, 7]
    """
    prime_factors = []
    i = 2
    while i * i <= n:
        if n % i:
            i += 1
        else:
            n //= i
            prime_factors.append(i)
    if n > 1:
        prime_factors.append(n)
    return prime_factors


if __name__ == "__main__":
    import doct
Time: 40.44s
Acceptance Rate: 0.9404761904761905
(tensor([0.9918]), tensor([0.9988]), tensor([0.9953]), tensor([0.9981]))
=> Regular sampling with target model
from typing import List


def remove_duplicates(numbers: List[int]) -> List[int]:
    """ From a list of integers, remove all elements that occur more than once.
    Keep order of elements left the same as in the input.
    >>> remove_duplicates([1, 2, 3, 2, 4])
    [1, 3, 4]
    """

    return [k for k in numbers if numbers.count(k) == 1]


if __name__ == "__main__":
    import doctest

    doctest.testmod()

Time: 20.55s
=> Speculative sampling with target model helped by draft model
from typing import List


def remove_duplicates(numbers: List[int]) -> List[int]:
    """ From a list of integers, remove all elements that occur more than once.
    Keep order of elements left the same as in the input.
    >>> remove_duplicates([1, 2, 3, 2, 4])
    [1, 3, 4]
    """

    return [k for k in numbers if numbers.count(k) == 1]


if __name__ == "__main__":
    import doctest

    doctest.testmod()
 import
Time: 20.49s
Acceptance Rate: 0.6964285714285714
(tensor([0.9903]), tensor([0.9973]), tensor([0.9938]), tensor([0.9966]))
=> Regular sampling with target model


def flip_case(string: str) -> str:
    """ For a given string, flip lowercase characters to uppercase and uppercase to lowercase.
    >>> flip_case('Hello')
    'hELLO'
    """
    return string.swapcase()


def flip_case2(string: str) -> str:
    """ For a given string, flip lowercase characters to uppercase and uppercase to lowercase.
    >>> flip_case2('Hello')
    'hELLO'
    """
    return ''.join(c.lower() if c.isupper() else c.upper() for c in string)
Time: 38.47s
=> Speculative sampling with target model helped by draft model


def flip_case(string: str) -> str:
    """ For a given string, flip lowercase characters to uppercase and uppercase to lowercase.
    >>> flip_case('Hello')
    'hELLO'
    """
    return string.swapcase()


def flip_case2(string: str) -> str:
    """ For a given string, flip lowercase characters to uppercase and uppercase to lowercase.
    >>> flip_case2('Hello')
    'hELLO'
    """
    return ''.join(c.lower() if c.isupper() else c.upper() for c in string)

Time: 27.20s
Acceptance Rate: 0.9625
(tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]))
=> Regular sampling with target model
from typing import List


def concatenate(strings: List[str]) -> str:
    """ Concatenate list of strings into a single string
    >>> concatenate([])
    ''
    >>> concatenate(['a', 'b', 'c'])
    'abc'
    """
    return ''.join(strings)


METADATA = {
    'author': 'jtara1',
    'dataset': 'test'
}


def hello_world():
    """Returns hello world
    >>> hello_world()
    'hello world'
    """
    return 'hello world'


def is_odd(num: int) -> bool:
    """Returns true
Time: 31.56s
=> Speculative sampling with target model helped by draft model
from typing import List


def concatenate(strings: List[str]) -> str:
    """ Concatenate list of strings into a single string
    >>> concatenate([])
    ''
    >>> concatenate(['a', 'b', 'c'])
    'abc'
    """
    return ''.join(strings)


METADATA = {
    'author': 'jt',
    'dataset': 'test'
}


def check(candidate):
    assert candidate([]) == ''
    assert candidate(['x', 'y', 'z']) == 'xyz'
    assert candidate(['x', 'y', 'z', 'w']) == 'xyzw'


if __name__ ==
Time: 29.99s
Acceptance Rate: 0.875
(tensor([0.8994]), tensor([0.9008]), tensor([0.9001]), tensor([0.9006]))
=> Regular sampling with target model
from typing import List


def filter_by_prefix(strings: List[str], prefix: str) -> List[str]:
    """ Filter an input list of strings only for ones that start with a given prefix.
    >>> filter_by_prefix([], 'a')
    []
    >>> filter_by_prefix(['abc', 'bcd', 'cde', 'array'], 'a')
    ['abc', 'array']
    """
    return list(filter(lambda s: s.startswith(prefix), strings))


if __name__ == "__main__":
    import doctest

    doctest.testmod()

Time: 16.23s
=> Speculative sampling with target model helped by draft model
from typing import List


def filter_by_prefix(strings: List[str], prefix: str) -> List[str]:
    """ Filter an input list of strings only for ones that start with a given prefix.
    >>> filter_by_prefix([], 'a')
    []
    >>> filter_by_prefix(['abc', 'bcd', 'cde', 'array'], 'a')
    ['abc', 'array']
    """
    return list(filter(lambda x: x.startswith(prefix), strings))


if __name__ == "__main__":
    import doctest

    doctest.testmod()

Time: 14.40s
Acceptance Rate: 1.0
(tensor([0.9961]), tensor([0.9961]), tensor([0.9961]), tensor([0.9961]))
=> Regular sampling with target model


def get_positive(l: list):
    """Return only positive numbers in the list.
    >>> get_positive([-1, 2, -4, 5, 6])
    [2, 5, 6]
    >>> get_positive([5, 3, -5, 2, -3, 3, 9, 0, 123, 1, -10])
    [5, 3, 2, 3, 9, 123, 1]
    """
    return [i for i in l if i > 0]


def get_negative(l: list):
    """Return only negative numbers in the list.
    >>> get_negative([-1, 2, -4, 5, 6])
    [-1, -4]
    >>> get_negative([5, 3, -5, 2, -3, 3, 9, 0
Time: 32.16s
=> Speculative sampling with target model helped by draft model


def get_positive(l: list):
    """Return only positive numbers in the list.
    >>> get_positive([-1, 2, -4, 5, 6])
    [2, 5, 6]
    >>> get_positive([5, 3, -5, 2, -3, 3, 9, 0, 123, 1, -10])
    [5, 3, 2, 3, 9, 123, 1]
    """
    return [i for i in l if i > 0]


def get_positive_clamp(l: list):
    """Return only positive numbers in the list.
    >>> get_positive_clamp([-1, 2, -4, 5, 6])
    [0, 2, 0, 5, 6]
    >>> get_positive_clamp([5
Time: 36.61s
Acceptance Rate: 0.8928571428571429
(tensor([0.9493]), tensor([0.9597]), tensor([0.9545]), tensor([0.9586]))
=> Regular sampling with target model


def is_prime(n):
    """Return true if a given number is prime, and false otherwise.
    >>> is_prime(6)
    False
    >>> is_prime(101)
    True
    >>> is_prime(11)
    True
    >>> is_prime(13441)
    True
    >>> is_prime(61)
    True
    >>> is_prime(4)
    False
    >>> is_prime(1)
    False
    """
    "*** YOUR CODE HERE ***"
    if n <= 1:
        return False
    else:
        for i in range(2, n):
            if n % i == 0:
                return False
        return True


def gcd(a, b):
    """Returns the greatest common divisor of a and b.
    Should be implemented using recursion.
    >>> gcd(
Time: 32.29s
=> Speculative sampling with target model helped by draft model


def is_prime(n):
    """Return true if a given number is prime, and false otherwise.
    >>> is_prime(6)
    False
    >>> is_prime(101)
    True
    >>> is_prime(11)
    True
    >>> is_prime(13441)
    True
    >>> is_prime(61)
    True
    >>> is_prime(4)
    False
    >>> is_prime(1)
    False
    """
    "*** YOUR CODE HERE ***"
    if n <= 1:
        return False
    else:
        for i in range(2, n):
            if n % i == 0:
                return False
        return True


def gcd(a, b):
    """Returns the greatest common divisor of a and b.
    Should be implemented using recursion.
    >>> gcd(34,
Time: 37.71s
Acceptance Rate: 0.875
(tensor([0.9947]), tensor([0.9981]), tensor([0.9964]), tensor([0.9978]))
=> Regular sampling with target model
import math


def poly(xs: list, x: float):
    """
    Evaluates polynomial with coefficients xs at point x.
    return xs[0] + xs[1] * x + xs[1] * x^2 + .... xs[n] * x^n
    """
    return sum([coeff * math.pow(x, i) for i, coeff in enumerate(xs)])


def find_zero(xs: list):
    """ xs are coefficients of a polynomial.
    find_zero find x such that poly(x) = 0.
    find_zero returns only only zero point, even if there are many.
    Moreover, find_zero only takes list xs having even number of coefficients
    and largest non zero coefficient as it guarantees
    a solution.
    >>> round(find_zero([1, 2]), 2) # f(x) = 1 + 2x
    -0.5
    >>> round(find_zero([-6, 11, -6, 1]), 2) # (x - 1) * (x - 2) * (x - 3) = -6 + 11x - 6x^2 + x^3
    1.0
    """
    assert len(xs) % 2 == 0
    assert xs[-1] != 0

    x = 5
    y = poly(xs, x)
    while y != 0:
        x = x - poly(xs, x) / derivative(xs, x)
        y = poly(xs, x)
    return x


def derivative(xs: list, x: float):
   
Time: 35.45s
=> Speculative sampling with target model helped by draft model
import math


def poly(xs: list, x: float):
    """
    Evaluates polynomial with coefficients xs at point x.
    return xs[0] + xs[1] * x + xs[1] * x^2 + .... xs[n] * x^n
    """
    return sum([coeff * math.pow(x, i) for i, coeff in enumerate(xs)])


def find_zero(xs: list):
    """ xs are coefficients of a polynomial.
    find_zero find x such that poly(x) = 0.
    find_zero returns only only zero point, even if there are many.
    Moreover, find_zero only takes list xs having even number of coefficients
    and largest non zero coefficient as it guarantees
    a solution.
    >>> round(find_zero([1, 2]), 2) # f(x) = 1 + 2x
    -0.5
    >>> round(find_zero([-6, 11, -6, 1]), 2) # (x - 1) * (x - 2) * (x - 3) = -6 + 11x - 6x^2 + x^3
    1.0
    """
    assert len(xs) % 2 == 0
    assert xs[-1] != 0

    x = 1.0
    whileTraceback (most recent call last):
  File "/home/ubuntu/llama-ssp/llamassp.py", line 297, in <module>
    acceptance_rate, sample_model_time, ssp_time, pred_results = show_comparative_speeds(text, model, draft_model, eval=True)
  File "/home/ubuntu/llama-ssp/llamassp.py", line 223, in show_comparative_speeds
    ssp_output_ids, total_count_accepted, total_count_generated = ssp_beam_greedy(model, draft_model, MAX_NEW_TOKENS,
  File "/home/ubuntu/llama-ssp/lssp/ssp.py", line 152, in ssp_beam_greedy
    input_ids, count_accepted = _beam_greedy_ssp_iteration(target_model, draft_model, input_ids, num_beams, K, display)
  File "/home/ubuntu/llama-ssp/lssp/ssp.py", line 46, in _beam_greedy_ssp_iteration
    target_logits = target_model(target_input).logits[:, -K-1:, :]
  File "/home/ubuntu/anaconda3/envs/llama_ssp/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/anaconda3/envs/llama_ssp/lib/python3.9/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/ubuntu/anaconda3/envs/llama_ssp/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 820, in forward
    outputs = self.model(
  File "/home/ubuntu/anaconda3/envs/llama_ssp/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/anaconda3/envs/llama_ssp/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 708, in forward
    layer_outputs = decoder_layer(
  File "/home/ubuntu/anaconda3/envs/llama_ssp/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/anaconda3/envs/llama_ssp/lib/python3.9/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/ubuntu/anaconda3/envs/llama_ssp/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 437, in forward
    hidden_states = self.mlp(hidden_states)
  File "/home/ubuntu/anaconda3/envs/llama_ssp/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/anaconda3/envs/llama_ssp/lib/python3.9/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/ubuntu/anaconda3/envs/llama_ssp/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 220, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/home/ubuntu/anaconda3/envs/llama_ssp/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/anaconda3/envs/llama_ssp/lib/python3.9/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/ubuntu/anaconda3/envs/llama_ssp/lib/python3.9/site-packages/bitsandbytes/nn/modules.py", line 441, in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
  File "/home/ubuntu/anaconda3/envs/llama_ssp/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py", line 563, in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
  File "/home/ubuntu/anaconda3/envs/llama_ssp/lib/python3.9/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/ubuntu/anaconda3/envs/llama_ssp/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py", line 404, in forward
    output = F.mm_dequant(out32, Sout32, SCA, state.SCB, bias=bias)
  File "/home/ubuntu/anaconda3/envs/llama_ssp/lib/python3.9/site-packages/bitsandbytes/functional.py", line 1816, in mm_dequant
    out = torch.empty(out_shape, dtype=torch.float16, device=A.device)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 0; 39.56 GiB total capacity; 35.57 GiB already allocated; 10.81 MiB free; 39.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
